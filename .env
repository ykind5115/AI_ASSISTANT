## CareMate 本地配置
## 说明：
## - 若 `MODEL_PATH` 指向存在的本地目录，启动时会优先从该目录加载模型（离线可用）。
## - 首次下载可用 `python scripts/download_model.py` 将模型拉取到该目录。

# 模型配置
# - 若 MODEL_PATH 指向 .gguf 文件或包含 .gguf 的目录，将使用 llama.cpp 推理（需要安装 llama-cpp-python）
# - 否则将使用 transformers/HuggingFace（MODEL_NAME 可为 repo id）
MODEL_NAME=Llama3-8B-Chinese-Chat-Q4_1

# 本地模型目录（目录内包含 .gguf 即可）
MODEL_PATH=data\models\Llama3-8B-Chinese-Chat-Q4_1

# 设备：cpu/cuda/auto
DEVICE=cuda

# 应用配置
DEBUG=False
API_HOST=127.0.0.1
API_PORT=8000
